{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import torch  \n",
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  BertForSequenceClassification,\n",
    "  AutoModelForMaskedLM,\n",
    "  pipeline\n",
    ")\n",
    "\n",
    "from common.data_utils import get_dataset\n",
    "from model.tokenizer import PhraseTokenizer\n",
    "from model.attacker import Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = BertForSequenceClassification.from_pretrained('./data/imdb/saved_model/imdb_bert_base_uncased_finetuned_normal').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "mlm_model = BertForMaskedLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_seq = \"What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\"\n",
    "entry = {'text': tgt_seq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'merge_noun_chunks', 'merge_entities']\n"
     ]
    }
   ],
   "source": [
    "phrase_tok = PhraseTokenizer()\n",
    "phrase_token_output = phrase_tok.tokenize(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'what a pity! frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " 'words': ['what',\n",
       "  'a',\n",
       "  'pity',\n",
       "  '!',\n",
       "  'frozen',\n",
       "  '2',\n",
       "  'is',\n",
       "  'bad',\n",
       "  'compared',\n",
       "  'to',\n",
       "  'its',\n",
       "  'predecessor',\n",
       "  ',',\n",
       "  'which',\n",
       "  'is',\n",
       "  'possibly',\n",
       "  'due',\n",
       "  'to',\n",
       "  'its',\n",
       "  'chaotic',\n",
       "  'production',\n",
       "  'process',\n",
       "  '.'],\n",
       " 'word_offsets': [(0, 4),\n",
       "  (5, 6),\n",
       "  (7, 11),\n",
       "  (11, 12),\n",
       "  (13, 19),\n",
       "  (20, 21),\n",
       "  (22, 24),\n",
       "  (25, 28),\n",
       "  (29, 37),\n",
       "  (38, 40),\n",
       "  (41, 44),\n",
       "  (45, 56),\n",
       "  (56, 57),\n",
       "  (58, 63),\n",
       "  (64, 66),\n",
       "  (67, 75),\n",
       "  (76, 79),\n",
       "  (80, 82),\n",
       "  (83, 86),\n",
       "  (87, 94),\n",
       "  (95, 105),\n",
       "  (106, 113),\n",
       "  (113, 114)],\n",
       " 'phrases': ['what a pity',\n",
       "  '!',\n",
       "  'frozen 2',\n",
       "  'is',\n",
       "  'bad',\n",
       "  'compared',\n",
       "  'to',\n",
       "  'its predecessor',\n",
       "  ',',\n",
       "  'which',\n",
       "  'is',\n",
       "  'possibly',\n",
       "  'due',\n",
       "  'to',\n",
       "  'its chaotic production process',\n",
       "  '.'],\n",
       " 'phrase_offsets': [(0, 11),\n",
       "  (11, 12),\n",
       "  (13, 21),\n",
       "  (22, 24),\n",
       "  (25, 28),\n",
       "  (29, 37),\n",
       "  (38, 40),\n",
       "  (41, 56),\n",
       "  (56, 57),\n",
       "  (58, 63),\n",
       "  (64, 66),\n",
       "  (67, 75),\n",
       "  (76, 79),\n",
       "  (80, 82),\n",
       "  (83, 113),\n",
       "  (113, 114)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_token_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = 0\n",
    "p_s = 0\n",
    "p_e = phrase_token_output['phrase_offsets'][p_i][1]\n",
    "p_len = 0\n",
    "phrase2word = []\n",
    "new_p = True\n",
    "word_count = 0\n",
    "for w_s, w_e in phrase_token_output['word_offsets']:\n",
    "    \n",
    "    if new_p:\n",
    "        p_s = word_count\n",
    "        new_p = False\n",
    "    \n",
    "    if w_e == p_e:\n",
    "        phrase2word.append([p_s, word_count+1])\n",
    "        new_p = True\n",
    "        p_i = min(p_i + 1, len(phrase_token_output['phrase_offsets']) - 1)\n",
    "        p_e = phrase_token_output['phrase_offsets'][p_i][1]\n",
    "    \n",
    "    word_count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3],\n",
       " [3, 4],\n",
       " [4, 6],\n",
       " [6, 7],\n",
       " [7, 8],\n",
       " [8, 9],\n",
       " [9, 10],\n",
       " [10, 12],\n",
       " [12, 13],\n",
       " [13, 14],\n",
       " [14, 15],\n",
       " [15, 16],\n",
       " [16, 17],\n",
       " [17, 18],\n",
       " [18, 22],\n",
       " [22, 23]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_masked_list = []\n",
    "word2char = phrase_token_output['word_offsets']\n",
    "\n",
    "for p_s, p_e in phrase2word:\n",
    "    if p_e - p_s >= 2:\n",
    "        c_s = word2char[ p_s ][0]\n",
    "        c_e = word2char[ p_e - 1][1]\n",
    "        \n",
    "        phrase_masked_list.append(tgt_seq[0:c_s] + ' [MASK] ' + tgt_seq[c_e:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " 'What a pity!  [MASK]  is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " 'What a pity! Frozen 2 is bad compared to  [MASK] , which is possibly due to its chaotic production process.',\n",
       " 'What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] .']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_masked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(phrase_masked_list, truncation=True, padding=True, return_token_type_ids=False, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,   999,  7708,  1016,  2003,  2919,  4102,  2000,  2049,\n",
       "          8646,  1010,  2029,  2003,  4298,  2349,  2000,  2049, 19633,  2537,\n",
       "          2832,  1012,   102,     0],\n",
       "        [  101,  2054,  1037, 12063,   999,   103,  2003,  2919,  4102,  2000,\n",
       "          2049,  8646,  1010,  2029,  2003,  4298,  2349,  2000,  2049, 19633,\n",
       "          2537,  2832,  1012,   102],\n",
       "        [  101,  2054,  1037, 12063,   999,  7708,  1016,  2003,  2919,  4102,\n",
       "          2000,   103,  1010,  2029,  2003,  4298,  2349,  2000,  2049, 19633,\n",
       "          2537,  2832,  1012,   102],\n",
       "        [  101,  2054,  1037, 12063,   999,  7708,  1016,  2003,  2919,  4102,\n",
       "          2000,  2049,  8646,  1010,  2029,  2003,  4298,  2349,  2000,   103,\n",
       "          1012,   102,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = encodings['input_ids'].to(device)\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  5, 11, 19], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'what',\n",
       " 'a',\n",
       " 'pity',\n",
       " '!',\n",
       " 'frozen',\n",
       " '2',\n",
       " 'is',\n",
       " 'bad',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'its',\n",
       " 'predecessor',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'possibly',\n",
       " 'due',\n",
       " 'to',\n",
       " '[MASK]',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encodings['input_ids'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 24, 30522])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  5, 11, 19], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_logits = torch.empty(token_logits.shape[0], token_logits.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,ind in enumerate(mask_token_index):\n",
    "    mask_token_logits[i] = token_logits[i, ind, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2175,  6289, 10930, 20643, 13184],\n",
       "        [ 2009,  2023,   999,  1010,  2029],\n",
       "        [ 7708,  2009,  8646,  1017,  2434],\n",
       "        [ 5821, 15657,  2287, 26968,  2023]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " go ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " ah ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " yo ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " yahoo ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " freeze ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "\n",
      "What a pity!  [MASK]  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  it  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  this  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  !  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  ,  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  which  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "\n",
      "What a pity! Frozen 2 is bad compared to  [MASK] , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  frozen , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  it , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  predecessor , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  3 , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  original , which is possibly due to its chaotic production process.\n",
      "\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  marketing .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  censorship .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  age .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  nostalgia .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  this .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (i,token) in enumerate(top_5_tokens):\n",
    "    print(phrase_masked_list[i])\n",
    "    for t in tokenizer.convert_ids_to_tokens(token):\n",
    "        print(phrase_masked_list[i].replace(tokenizer.mask_token, t))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-attack)",
   "language": "python",
   "name": "bert-attack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
