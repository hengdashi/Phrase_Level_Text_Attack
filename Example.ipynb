{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import torch  \n",
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  BertForSequenceClassification,\n",
    "  AutoModelForMaskedLM,\n",
    "  pipeline\n",
    ")\n",
    "\n",
    "from common.data_utils import get_dataset\n",
    "from model.tokenizer import PhraseTokenizer\n",
    "from model.attacker import Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = BertForSequenceClassification.from_pretrained('./data/imdb/saved_model/imdb_bert_base_uncased_finetuned_normal').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "mlm_model = BertForMaskedLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_seq = \"What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\"\n",
    "entry = {'text': tgt_seq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'merge_noun_chunks', 'merge_entities']\n"
     ]
    }
   ],
   "source": [
    "phrase_tok = PhraseTokenizer()\n",
    "phrase_token_output = phrase_tok.tokenize(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'what a pity! frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " 'words': ['what',\n",
       "  'a',\n",
       "  'pity',\n",
       "  '!',\n",
       "  'frozen',\n",
       "  '2',\n",
       "  'is',\n",
       "  'bad',\n",
       "  'compared',\n",
       "  'to',\n",
       "  'its',\n",
       "  'predecessor',\n",
       "  ',',\n",
       "  'which',\n",
       "  'is',\n",
       "  'possibly',\n",
       "  'due',\n",
       "  'to',\n",
       "  'its',\n",
       "  'chaotic',\n",
       "  'production',\n",
       "  'process',\n",
       "  '.'],\n",
       " 'word_offsets': [(0, 4),\n",
       "  (5, 6),\n",
       "  (7, 11),\n",
       "  (11, 12),\n",
       "  (13, 19),\n",
       "  (20, 21),\n",
       "  (22, 24),\n",
       "  (25, 28),\n",
       "  (29, 37),\n",
       "  (38, 40),\n",
       "  (41, 44),\n",
       "  (45, 56),\n",
       "  (56, 57),\n",
       "  (58, 63),\n",
       "  (64, 66),\n",
       "  (67, 75),\n",
       "  (76, 79),\n",
       "  (80, 82),\n",
       "  (83, 86),\n",
       "  (87, 94),\n",
       "  (95, 105),\n",
       "  (106, 113),\n",
       "  (113, 114)],\n",
       " 'phrases': ['what a pity',\n",
       "  '!',\n",
       "  'frozen 2',\n",
       "  'is',\n",
       "  'bad',\n",
       "  'compared',\n",
       "  'to',\n",
       "  'its predecessor',\n",
       "  ',',\n",
       "  'which',\n",
       "  'is',\n",
       "  'possibly',\n",
       "  'due',\n",
       "  'to',\n",
       "  'its chaotic production process',\n",
       "  '.'],\n",
       " 'phrase_offsets': [(0, 11),\n",
       "  (11, 12),\n",
       "  (13, 21),\n",
       "  (22, 24),\n",
       "  (25, 28),\n",
       "  (29, 37),\n",
       "  (38, 40),\n",
       "  (41, 56),\n",
       "  (56, 57),\n",
       "  (58, 63),\n",
       "  (64, 66),\n",
       "  (67, 75),\n",
       "  (76, 79),\n",
       "  (80, 82),\n",
       "  (83, 113),\n",
       "  (113, 114)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_token_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = 0\n",
    "p_s = 0\n",
    "p_e = phrase_token_output['phrase_offsets'][p_i][1]\n",
    "p_len = 0\n",
    "phrase2word = []\n",
    "new_p = True\n",
    "word_count = 0\n",
    "for w_s, w_e in phrase_token_output['word_offsets']:\n",
    "    \n",
    "    if new_p:\n",
    "        p_s = word_count\n",
    "        new_p = False\n",
    "    \n",
    "    if w_e == p_e:\n",
    "        phrase2word.append([p_s, word_count+1])\n",
    "        new_p = True\n",
    "        p_i = min(p_i + 1, len(phrase_token_output['phrase_offsets']) - 1)\n",
    "        p_e = phrase_token_output['phrase_offsets'][p_i][1]\n",
    "    \n",
    "    word_count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3],\n",
       " [3, 4],\n",
       " [4, 6],\n",
       " [6, 7],\n",
       " [7, 8],\n",
       " [8, 9],\n",
       " [9, 10],\n",
       " [10, 12],\n",
       " [12, 13],\n",
       " [13, 14],\n",
       " [14, 15],\n",
       " [15, 16],\n",
       " [16, 17],\n",
       " [17, 18],\n",
       " [18, 22],\n",
       " [22, 23]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_masked_list = []\n",
    "word2char = phrase_token_output['word_offsets']\n",
    "\n",
    "mask_index_list = []\n",
    "mask_count = 0\n",
    "for p_s, p_e in phrase2word:\n",
    "    if p_e - p_s >= 2:\n",
    "        c_s = word2char[ p_s ][0]\n",
    "        c_e = word2char[ p_e - 1][1]\n",
    "        \n",
    "        mask_len = p_e - p_s\n",
    "        phrase_masked_list.append(tgt_seq[0:c_s] + ' [MASK]' * mask_len + ' ' + tgt_seq[c_e:])\n",
    "        mask_index_list.append([mask_count, mask_count + mask_len])\n",
    "        mask_count += mask_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' [MASK] [MASK] [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " 'What a pity!  [MASK] [MASK]  is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " 'What a pity! Frozen 2 is bad compared to  [MASK] [MASK] , which is possibly due to its chaotic production process.',\n",
       " 'What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] [MASK] [MASK] [MASK] .']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_masked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(phrase_masked_list, truncation=True, padding=True, return_token_type_ids=False, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,   103,   103,   999,  7708,  1016,  2003,  2919,  4102,\n",
       "          2000,  2049,  8646,  1010,  2029,  2003,  4298,  2349,  2000,  2049,\n",
       "         19633,  2537,  2832,  1012,   102],\n",
       "        [  101,  2054,  1037, 12063,   999,   103,   103,  2003,  2919,  4102,\n",
       "          2000,  2049,  8646,  1010,  2029,  2003,  4298,  2349,  2000,  2049,\n",
       "         19633,  2537,  2832,  1012,   102],\n",
       "        [  101,  2054,  1037, 12063,   999,  7708,  1016,  2003,  2919,  4102,\n",
       "          2000,   103,   103,  1010,  2029,  2003,  4298,  2349,  2000,  2049,\n",
       "         19633,  2537,  2832,  1012,   102],\n",
       "        [  101,  2054,  1037, 12063,   999,  7708,  1016,  2003,  2919,  4102,\n",
       "          2000,  2049,  8646,  1010,  2029,  2003,  4298,  2349,  2000,   103,\n",
       "           103,   103,   103,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = encodings['input_ids'].to(device)\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  5,  6, 11, 12, 19, 20, 21, 22], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'what',\n",
       " 'a',\n",
       " 'pity',\n",
       " '!',\n",
       " 'frozen',\n",
       " '2',\n",
       " 'is',\n",
       " 'bad',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'its',\n",
       " 'predecessor',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'possibly',\n",
       " 'due',\n",
       " 'to',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '[MASK]',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encodings['input_ids'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 25, 30522])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_logits = mlm_model(inputs, attention_mask=encodings['attention_mask'].to(device)).logits\n",
    "token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_logits = torch.empty(len(mask_token_index), token_logits.shape[2])\n",
    "\n",
    "for i,ind in enumerate(mask_index_list):\n",
    "    li_s = mask_index_list[i][0]\n",
    "    li_e = mask_index_list[i][1]\n",
    "    ind_s = mask_token_index[li_s]\n",
    "    ind_e = mask_token_index[li_e - 1] + 1\n",
    "    #print(li_s, li_e, ind_s, ind_e)\n",
    "        \n",
    "    mask_token_logits[li_s:li_e] = token_logits[i, ind_s:ind_e, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3], [3, 5], [5, 7], [7, 11]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 30522])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices\n",
    "top_5_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1996, 14021,  2196,  8038,  9805],\n",
       "        [  999,  1011, 23644,  7507,  1010],\n",
       "        [ 2080,  2050, 10930,  2017,  2213],\n",
       "        [ 1996,  2023,  2049,  2008,  2117],\n",
       "        [ 2143,  8297,  3185,  2201,  2208],\n",
       "        [ 2049,  7708,  1996,  3025,  3256],\n",
       "        [ 8646,  2434,  1015, 16372,  2034],\n",
       "        [ 1996,  2049,  1037,  3768,  2019],\n",
       "        [ 3768,  1997,  1043,  2058,  3532],\n",
       "        [ 1997, 15909,  1998,  1999,  2208],\n",
       "        [ 2015, 11247,  4180,  3314,  3194]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3], [3, 5], [5, 7], [7, 11]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([125, 3])\n",
      "['the', 'sh', 'never', 'ya', 'yu']\n",
      "['!', '-', '##hh', '##cha', ',']\n",
      "['##o', '##a', 'yo', 'you', '##m']\n",
      " [MASK] [MASK] [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " the !m ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " thehha ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " thechao ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " thecha you ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " the ,o ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "\n",
      "torch.Size([25, 2])\n",
      "['the', 'this', 'its', 'that', 'second']\n",
      "['film', 'sequel', 'movie', 'album', 'game']\n",
      "What a pity!  [MASK] [MASK]  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  the film  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  the movie  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  that game  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  this sequel  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  that film  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "\n",
      "torch.Size([25, 2])\n",
      "['its', 'frozen', 'the', 'previous', 'ice']\n",
      "['predecessor', 'original', '1', 'predecessors', 'first']\n",
      "What a pity! Frozen 2 is bad compared to  [MASK] [MASK] , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  the first , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  the predecessor , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  its predecessor , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  the 1 , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  ice first , which is possibly due to its chaotic production process.\n",
      "\n",
      "torch.Size([625, 4])\n",
      "['the', 'its', 'a', 'lack', 'an']\n",
      "['lack', 'of', 'g', 'over', 'poor']\n",
      "['of', '##lit', 'and', 'in', 'game']\n",
      "['##s', 'gameplay', 'content', 'issues', 'engine']\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] [MASK] [MASK] [MASK] .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the lack of issues .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the of and issues .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the lack of engine .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the of of issues .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  the lack of gameplay .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (i, (p_s, p_e)) in enumerate(mask_index_list):\n",
    "    cur_phrase = ''\n",
    "    substitutes = top_5_tokens[p_s:p_e]\n",
    "    final_words = get_substitutes(substitutes, tokenizer, mlm_model)\n",
    "    for s in substitutes:\n",
    "        print(tokenizer.convert_ids_to_tokens(s))\n",
    "    \n",
    "    print(phrase_masked_list[i])\n",
    "    for w in final_words[:5]:\n",
    "        print(phrase_masked_list[i].replace((f' {tokenizer.mask_token}' * (p_e - p_s))[1:], w))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_substitutes(substitutes, tokenizer, mlm_model):\n",
    "    # all substitutes  list of list of token-id (all candidates)\n",
    "    c_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    word_list = []\n",
    "\n",
    "    # find all possible candidates \n",
    "    all_substitutes = []\n",
    "    for i in range(substitutes.size(0)):\n",
    "        if len(all_substitutes) == 0:\n",
    "            lev_i = substitutes[i]\n",
    "            all_substitutes = [[int(c)] for c in lev_i]\n",
    "        else:\n",
    "            lev_i = []\n",
    "            for all_sub in all_substitutes:\n",
    "                for j in substitutes[i]:\n",
    "                    lev_i.append(all_sub + [int(j)])\n",
    "            all_substitutes = lev_i\n",
    "\n",
    "    # all_substitutes = all_substitutes[:24]\n",
    "    all_substitutes = torch.tensor(all_substitutes) # [ N, L ]\n",
    "    all_substitutes = all_substitutes.to(device)\n",
    "    \n",
    "    print(all_substitutes.shape) # (K ^ t, K)\n",
    "\n",
    "    N, L = all_substitutes.size()\n",
    "    word_predictions = mlm_model(all_substitutes)[0] # N L vocab-size\n",
    "    ppl = c_loss(word_predictions.view(N*L, -1), all_substitutes.view(-1)) # [ N*L ] \n",
    "    ppl = torch.exp(torch.mean(ppl.view(N, L), dim=-1)) # N  \n",
    "    \n",
    "    _, word_list = torch.sort(ppl)\n",
    "    word_list = [all_substitutes[i] for i in word_list]\n",
    "    final_words = []\n",
    "    for word in word_list[:24]:\n",
    "        tokens = [tokenizer._convert_id_to_token(int(i)) for i in word]\n",
    "        text = tokenizer.convert_tokens_to_string(tokens)\n",
    "        final_words.append(text)\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [MASK] [MASK] [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_masked_list[0].replace(' [MASK]' * (p_e - p_s) + ' ', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " go ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " ah ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " yo ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " yahoo ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      " freeze ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "\n",
      "What a pity!  [MASK]  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  it  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  this  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  !  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  ,  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "What a pity!  which  is bad compared to its predecessor, which is possibly due to its chaotic production process.\n",
      "\n",
      "What a pity! Frozen 2 is bad compared to  [MASK] , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  frozen , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  it , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  predecessor , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  3 , which is possibly due to its chaotic production process.\n",
      "What a pity! Frozen 2 is bad compared to  original , which is possibly due to its chaotic production process.\n",
      "\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  marketing .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  censorship .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  age .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  nostalgia .\n",
      "What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  this .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (i,token) in enumerate(top_5_tokens):\n",
    "    print(phrase_masked_list[i])\n",
    "    for t in tokenizer.convert_ids_to_tokens(token):\n",
    "        print(phrase_masked_list[i].replace(tokenizer.mask_token, t))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-attack)",
   "language": "python",
   "name": "bert-attack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
