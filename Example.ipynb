{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import torch  \n",
    "import datasets\n",
    "from datasets import concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "  AutoTokenizer,\n",
    "  BertForSequenceClassification,\n",
    "  AutoModelForMaskedLM,\n",
    "  pipeline\n",
    ")\n",
    "\n",
    "from common.data_utils import get_dataset\n",
    "from model.tokenizer import PhraseTokenizer\n",
    "from model.attacker import Attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = BertForSequenceClassification.from_pretrained('./data/imdb/saved_model/imdb_bert_base_uncased_finetuned_normal').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-large-uncased-whole-word-masking\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "mlm_model = BertForMaskedLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_seq = \"What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.\"\n",
    "entry = {'text': tgt_seq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'merge_noun_chunks', 'merge_entities']\n"
     ]
    }
   ],
   "source": [
    "phrase_tok = PhraseTokenizer()\n",
    "phrase_token_output = phrase_tok.tokenize(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'what a pity! frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " 'words': ['what',\n",
       "  'a',\n",
       "  'pity',\n",
       "  '!',\n",
       "  'frozen',\n",
       "  '2',\n",
       "  'is',\n",
       "  'bad',\n",
       "  'compared',\n",
       "  'to',\n",
       "  'its',\n",
       "  'predecessor',\n",
       "  ',',\n",
       "  'which',\n",
       "  'is',\n",
       "  'possibly',\n",
       "  'due',\n",
       "  'to',\n",
       "  'its',\n",
       "  'chaotic',\n",
       "  'production',\n",
       "  'process',\n",
       "  '.'],\n",
       " 'word_offsets': [(0, 4),\n",
       "  (5, 6),\n",
       "  (7, 11),\n",
       "  (11, 12),\n",
       "  (13, 19),\n",
       "  (20, 21),\n",
       "  (22, 24),\n",
       "  (25, 28),\n",
       "  (29, 37),\n",
       "  (38, 40),\n",
       "  (41, 44),\n",
       "  (45, 56),\n",
       "  (56, 57),\n",
       "  (58, 63),\n",
       "  (64, 66),\n",
       "  (67, 75),\n",
       "  (76, 79),\n",
       "  (80, 82),\n",
       "  (83, 86),\n",
       "  (87, 94),\n",
       "  (95, 105),\n",
       "  (106, 113),\n",
       "  (113, 114)],\n",
       " 'phrases': ['what a pity',\n",
       "  '!',\n",
       "  'frozen 2',\n",
       "  'is',\n",
       "  'bad',\n",
       "  'compared',\n",
       "  'to',\n",
       "  'its predecessor',\n",
       "  ',',\n",
       "  'which',\n",
       "  'is',\n",
       "  'possibly',\n",
       "  'due',\n",
       "  'to',\n",
       "  'its chaotic production process',\n",
       "  '.'],\n",
       " 'phrase_offsets': [(0, 11),\n",
       "  (11, 12),\n",
       "  (13, 21),\n",
       "  (22, 24),\n",
       "  (25, 28),\n",
       "  (29, 37),\n",
       "  (38, 40),\n",
       "  (41, 56),\n",
       "  (56, 57),\n",
       "  (58, 63),\n",
       "  (64, 66),\n",
       "  (67, 75),\n",
       "  (76, 79),\n",
       "  (80, 82),\n",
       "  (83, 113),\n",
       "  (113, 114)]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_token_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_i = 0\n",
    "p_s = 0\n",
    "p_e = phrase_token_output['phrase_offsets'][p_i][1]\n",
    "p_len = 0\n",
    "phrase2word = []\n",
    "new_p = True\n",
    "word_count = 0\n",
    "for w_s, w_e in phrase_token_output['word_offsets']:\n",
    "    \n",
    "    if new_p:\n",
    "        p_s = word_count\n",
    "        new_p = False\n",
    "    \n",
    "    if w_e == p_e:\n",
    "        phrase2word.append([p_s, word_count+1])\n",
    "        new_p = True\n",
    "        p_i = min(p_i + 1, len(phrase_token_output['phrase_offsets']) - 1)\n",
    "        p_e = phrase_token_output['phrase_offsets'][p_i][1]\n",
    "    \n",
    "    word_count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3],\n",
       " [3, 4],\n",
       " [4, 6],\n",
       " [6, 7],\n",
       " [7, 8],\n",
       " [8, 9],\n",
       " [9, 10],\n",
       " [10, 12],\n",
       " [12, 13],\n",
       " [13, 14],\n",
       " [14, 15],\n",
       " [15, 16],\n",
       " [16, 17],\n",
       " [17, 18],\n",
       " [18, 22],\n",
       " [22, 23]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_masked_list = []\n",
    "word2char = phrase_token_output['word_offsets']\n",
    "\n",
    "for p_s, p_e in phrase2word:\n",
    "    if p_e - p_s >= 2:\n",
    "        c_s = word2char[ p_s ][0]\n",
    "        c_e = word2char[ p_e - 1][1]\n",
    "        \n",
    "        phrase_masked_list.append(tgt_seq[0:c_s] + ' [MASK] ' + tgt_seq[c_e:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' [MASK] ! Frozen 2 is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " 'What a pity!  [MASK]  is bad compared to its predecessor, which is possibly due to its chaotic production process.',\n",
       " 'What a pity! Frozen 2 is bad compared to  [MASK] , which is possibly due to its chaotic production process.',\n",
       " 'What a pity! Frozen 2 is bad compared to its predecessor, which is possibly due to  [MASK] .']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_masked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(phrase_masked_list, return_tensors=\"pt\").to(device)\n",
    "mask_token_index = torch.where(inputs == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(phrase_masked_list, truncation=True, padding=True, return_token_type_ids=False, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,   999,  7708,  1016,  2003,  2919,  4102,  2000,  2049,\n",
       "          8646,  1010,  2029,  2003,  4298,  2349,  2000,  2049, 19633,  2537,\n",
       "          2832,  1012,   102,     0],\n",
       "        [  101,  2054,  1037, 12063,   999,   103,  2003,  2919,  4102,  2000,\n",
       "          2049,  8646,  1010,  2029,  2003,  4298,  2349,  2000,  2049, 19633,\n",
       "          2537,  2832,  1012,   102],\n",
       "        [  101,  2054,  1037, 12063,   999,  7708,  1016,  2003,  2919,  4102,\n",
       "          2000,   103,  1010,  2029,  2003,  4298,  2349,  2000,  2049, 19633,\n",
       "          2537,  2832,  1012,   102],\n",
       "        [  101,  2054,  1037, 12063,   999,  7708,  1016,  2003,  2919,  4102,\n",
       "          2000,  2049,  8646,  1010,  2029,  2003,  4298,  2349,  2000,   103,\n",
       "          1012,   102,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_index = torch.where(encodings['input_ids'] == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  5, 11, 19])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'what',\n",
       " 'a',\n",
       " 'pity',\n",
       " '!',\n",
       " 'frozen',\n",
       " '2',\n",
       " 'is',\n",
       " 'bad',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'its',\n",
       " 'predecessor',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'possibly',\n",
       " 'due',\n",
       " 'to',\n",
       " '[MASK]',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encodings['input_ids'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help smaller out carbon footprint.\n",
      "Distilled models are cheaper than the models they mimic. Using them instead of the large versions would help cheaper out carbon footprint.\n",
      "Distilled models are simpler than the models they mimic. Using them instead of the large versions would help simpler out carbon footprint.\n",
      "Distilled models are larger than the models they mimic. Using them instead of the large versions would help larger out carbon footprint.\n",
      "Distilled models are lighter than the models they mimic. Using them instead of the large versions would help lighter out carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "token_logits = mlm_model(inputs).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{tokenizer.mask_token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'merge_noun_chunks', 'merge_entities']\n"
     ]
    }
   ],
   "source": [
    "phrase_tok = PhraseTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = {}\n",
    "entry['text'] = sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['di',\n",
       " '##sti',\n",
       " '##lled',\n",
       " 'models',\n",
       " 'are',\n",
       " '[MASK]',\n",
       " 'than',\n",
       " 'the',\n",
       " 'models',\n",
       " 'they',\n",
       " 'mimic',\n",
       " '.',\n",
       " 'using',\n",
       " 'them',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'the',\n",
       " 'large',\n",
       " 'versions',\n",
       " 'would',\n",
       " 'help',\n",
       " '[MASK]',\n",
       " 'out',\n",
       " 'carbon',\n",
       " 'footprint',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'distilled models are [mask] than the models they mimic. using them instead of the large versions would help [mask] out carbon footprint.',\n",
       " 'words': ['distilled',\n",
       "  'models',\n",
       "  'are',\n",
       "  '[',\n",
       "  'mask',\n",
       "  ']',\n",
       "  'than',\n",
       "  'the',\n",
       "  'models',\n",
       "  'they',\n",
       "  'mimic',\n",
       "  '.',\n",
       "  'using',\n",
       "  'them',\n",
       "  'instead',\n",
       "  'of',\n",
       "  'the',\n",
       "  'large',\n",
       "  'versions',\n",
       "  'would',\n",
       "  'help',\n",
       "  '[',\n",
       "  'mask',\n",
       "  ']',\n",
       "  'out',\n",
       "  'carbon',\n",
       "  'footprint',\n",
       "  '.'],\n",
       " 'word_offsets': [(0, 9),\n",
       "  (10, 16),\n",
       "  (17, 20),\n",
       "  (21, 22),\n",
       "  (22, 26),\n",
       "  (26, 27),\n",
       "  (28, 32),\n",
       "  (33, 36),\n",
       "  (37, 43),\n",
       "  (44, 48),\n",
       "  (49, 54),\n",
       "  (54, 55),\n",
       "  (56, 61),\n",
       "  (62, 66),\n",
       "  (67, 74),\n",
       "  (75, 77),\n",
       "  (78, 81),\n",
       "  (82, 87),\n",
       "  (88, 96),\n",
       "  (97, 102),\n",
       "  (103, 107),\n",
       "  (108, 109),\n",
       "  (109, 113),\n",
       "  (113, 114),\n",
       "  (115, 118),\n",
       "  (119, 125),\n",
       "  (126, 135),\n",
       "  (135, 136)],\n",
       " 'phrases': ['distilled models',\n",
       "  'are',\n",
       "  '[mask',\n",
       "  ']',\n",
       "  'than',\n",
       "  'the models',\n",
       "  'they',\n",
       "  'mimic',\n",
       "  '.',\n",
       "  'using',\n",
       "  'them',\n",
       "  'instead',\n",
       "  'of',\n",
       "  'the large versions',\n",
       "  'would',\n",
       "  'help',\n",
       "  '[',\n",
       "  'mask',\n",
       "  ']',\n",
       "  'out',\n",
       "  'carbon footprint',\n",
       "  '.'],\n",
       " 'phrase_offsets': [(0, 16),\n",
       "  (17, 20),\n",
       "  (21, 26),\n",
       "  (26, 27),\n",
       "  (28, 32),\n",
       "  (33, 43),\n",
       "  (44, 48),\n",
       "  (49, 54),\n",
       "  (54, 55),\n",
       "  (56, 61),\n",
       "  (62, 66),\n",
       "  (67, 74),\n",
       "  (75, 77),\n",
       "  (78, 96),\n",
       "  (97, 102),\n",
       "  (103, 107),\n",
       "  (108, 109),\n",
       "  (109, 113),\n",
       "  (113, 114),\n",
       "  (115, 118),\n",
       "  (119, 135),\n",
       "  (135, 136)]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_tok.tokenize(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-attack)",
   "language": "python",
   "name": "bert-attack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
